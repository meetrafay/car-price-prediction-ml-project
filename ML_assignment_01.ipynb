{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Car Price Prediction â€” Machine Learning Project\n",
        "\n",
        "## ðŸ“Œ Objective\n",
        "The goal of this project is to build a predictive model for car prices.  \n",
        "Weâ€™ll walk through a complete Machine Learning pipeline:\n",
        "- Load and clean the dataset\n",
        "- Perform Exploratory Data Analysis (EDA)\n",
        "- Handle missing values and outliers\n",
        "- Encode categorical features\n",
        "- Train and evaluate regression models\n",
        "- Compare model performances and feature importance\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RZxufaR7k5f4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cgxkxuh0FEX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "4QqW4OtglfiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Data Exploration (EDA)\n",
        "\n",
        "* The dataset was loaded using the Pandas library (`pd.read_csv('your_dataset.csv')`).\n",
        "* The first five rows were displayed using `.head()`.\n",
        "* Missing values were checked using `.isnull().sum()`.\n",
        "* Data types were identified using `.dtypes`, distinguishing numerical and categorical features."
      ],
      "metadata": {
        "id": "KOsLWuzvk96w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/cars_price.csv')"
      ],
      "metadata": {
        "id": "tr32f1El0RIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "jcdQ8OE-0cZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "AgpQpFO80fZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "i8ukvIAf0kXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "i-_s0NBS0zan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "WCol0OOHKkSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "Xm-LvdKLL0gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate null percentage of each columns.\n",
        "\n",
        "null_percentage = (df.isnull().sum()/len(df)) * 100\n",
        "null_percentage.sort_values(ascending=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N-BSkRvXGZmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with greater than 40% null values\n",
        "\n",
        "remove_cols = null_percentage[null_percentage > 40].keys().tolist()\n",
        "df.drop(remove_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "N_C3CYP-HKab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj_cols = df.select_dtypes(include='object').columns"
      ],
      "metadata": {
        "id": "O36ym2wk75aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore objects columns\n",
        "\n",
        "for col in obj_cols:\n",
        "  print(df[col].unique())\n",
        "  print(len(df[col].value_counts()))\n",
        "  print(df[col].value_counts())\n",
        "  print(\"================\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bpBxd2UY8Y7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Garbage Values and Data Type Conversion\n",
        "\n",
        "This step involves cleaning the dataset by identifying and replacing any \"garbage\" values with `NaN` (Not a Number), which is the standard way to represent missing data in Pandas. Additionally, the data types of the columns will be reviewed and changed if they are not appropriate for the data they contain or for subsequent analysis and modeling."
      ],
      "metadata": {
        "id": "An9_2iMOn7on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"normalized-losses\"] = df[\"normalized-losses\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "Mr2VHEFYKHwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"normalized-losses\"] = pd.to_numeric(df[\"normalized-losses\"], errors='coerce')"
      ],
      "metadata": {
        "id": "CsGgxtNxM5we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"num-of-doors\"] = df[\"num-of-doors\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "3cREcBK7OkiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"bore\"] = df[\"bore\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "gil-ZbAJPZuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"bore\"] = pd.to_numeric(df[\"bore\"], errors='coerce')"
      ],
      "metadata": {
        "id": "iMS97SkAQJ6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"stroke\"] = df[\"stroke\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "z5l6ZncH3cbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"stroke\"] = pd.to_numeric(df[\"stroke\"], errors='coerce')"
      ],
      "metadata": {
        "id": "sBsm23Cw3k_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"horsepower\"] = df[\"horsepower\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "RYo3vSo_4KOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"horsepower\"] = pd.to_numeric(df[\"horsepower\"], errors='coerce')"
      ],
      "metadata": {
        "id": "wvdWC6rB4WzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"peak-rpm\"] = df[\"peak-rpm\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "FDTTw4aK4aDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"peak-rpm\"] = pd.to_numeric(df[\"peak-rpm\"], errors='coerce')"
      ],
      "metadata": {
        "id": "bvG4LEhX4nhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"price\"] = df[\"price\"].replace('?', np.nan)"
      ],
      "metadata": {
        "id": "g8Pp72RQ4r3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"price\"] = pd.to_numeric(df[\"price\"], errors='coerce')"
      ],
      "metadata": {
        "id": "gqZQ0eWf460G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where target value \"price\" value is null.\n",
        "\n",
        "df.dropna(subset=[\"price\"], inplace=True)"
      ],
      "metadata": {
        "id": "tH_HTBa4rrM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring and Filling NaN Values in Object Columns\n",
        "\n",
        "This step focuses on examining the count of missing values (`NaN`) specifically within columns having an `object` data type (which typically represent categorical or string data). If the number of `NaN` values in these columns is below a certain threshold, we will proceed to fill them using an appropriate strategy."
      ],
      "metadata": {
        "id": "xuktiHUVqL8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_cols = df.select_dtypes(include='O').columns\n",
        "for col in obj_cols:\n",
        "  print(col)\n",
        "  print(df[col].isna().sum())\n",
        "  print(len(df[col]))\n",
        "  print(df[col].value_counts())\n",
        "  print(\"=====================\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R46uc7Bo4_hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_categorical_missing_values(cols):\n",
        "  for col in obj_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "  return None"
      ],
      "metadata": {
        "id": "xeVSAELO5F7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_categorical_missing_values(obj_cols)"
      ],
      "metadata": {
        "id": "ZBKmjdtR5diw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = df.select_dtypes(exclude='O').columns\n",
        "num_cols"
      ],
      "metadata": {
        "id": "AVdgG9XX9fii",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate numerical categorical colmuns\n",
        "\n",
        "def separate_categorical_col_by_value_count(cols, threshold=10):\n",
        "    rem_cat_cols = []\n",
        "    for col in cols:\n",
        "        if df[col].nunique() <= threshold:\n",
        "            rem_cat_cols.append(col)\n",
        "    return rem_cat_cols"
      ],
      "metadata": {
        "id": "QjW-I1ovVi8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rem_cat_cols = separate_categorical_col_by_value_count(num_cols)\n",
        "rem_cat_cols"
      ],
      "metadata": {
        "id": "WVmVJfLTV7jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explore columns\n",
        "\n",
        "for col in rem_cat_cols:\n",
        "  print(col)\n",
        "  print(df[col].unique())\n",
        "  print(df[col].value_counts())\n",
        "  print(df[col].isna().sum())\n",
        "  print(\"=====================\")"
      ],
      "metadata": {
        "id": "aNjzbVzFXK7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert negative values to positive\n",
        "\n",
        "df['symboling'] = df['symboling'].abs()"
      ],
      "metadata": {
        "id": "wq8qxWKsXuE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_categorical_missing_values(rem_cat_cols)"
      ],
      "metadata": {
        "id": "T8Hi3QwgYkVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Exploring Visualizations for Categorical Data\n",
        "\n",
        "This step involves generating and analyzing various visualizations to understand the distribution and patterns within our categorical features and their relationship with the target variable (price). Visualizations help in gaining insights into the different categories, their frequencies, and how they might influence car prices."
      ],
      "metadata": {
        "id": "vccZNbj2t21O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_cat_cols = list(df.select_dtypes(include='O').columns) + rem_cat_cols"
      ],
      "metadata": {
        "id": "Zy1Y-Qs9t0Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Jpsd-LMRtqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a box plot of each categorical colmuns to analyze the impact on target variable 'price'\n",
        "n_cols = len(all_cat_cols)\n",
        "cols_per_row = 2\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))\n",
        "\n",
        "for index, col in enumerate(all_cat_cols):\n",
        "    plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "    sns.boxplot(x=col, y='price', data=df)\n",
        "    plt.title(f'Box Plot of price vs. {col} Column')\n",
        "    plt.xlabel(f'{col}')\n",
        "    plt.ylabel('Target Variable Price')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bV06zI1GYzqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Non-Impacting Categorical Columns\n",
        "\n",
        "Based on our analysis (likely through visualizations or statistical tests), the following categorical columns were identified as not having a significant impact on the target variable ('price'):"
      ],
      "metadata": {
        "id": "E7sJ0SYdu1sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop irrelevent categorical columns\n",
        "\n",
        "rem_col = [col for col in all_cat_cols if col in [\"fuel-type\",\"num-of-doors\",\"fuel-system\",\"symboling\"]]\n",
        "rem_col"
      ],
      "metadata": {
        "id": "pLsuA4AtkqUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped_list = df.drop(rem_col, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZHkRkhw-lSmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Numerical Columns\n",
        "\n",
        "This step involves a detailed exploration of the numerical features in the dataset to understand their statistical properties and the extent of missing values. This analysis helps in identifying potential issues, understanding the distribution of the data, and informing subsequent preprocessing steps."
      ],
      "metadata": {
        "id": "up7_teQ7w2fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = df.select_dtypes(exclude='O').columns"
      ],
      "metadata": {
        "id": "RMdjUfU3oktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_numerial_col_by_value_count(cols, threshold=10):\n",
        "    num_cols = []\n",
        "    for col in cols:\n",
        "        if df[col].nunique() > threshold:\n",
        "            num_cols.append(col)\n",
        "    return num_cols"
      ],
      "metadata": {
        "id": "Q_KOzaXRwiin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = separate_numerial_col_by_value_count(num_cols)"
      ],
      "metadata": {
        "id": "G03QkeCbxC-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "  print(col)\n",
        "  print(df[col].isna().sum())\n",
        "  print(df[col].describe())\n",
        "  print(\"=====================\")"
      ],
      "metadata": {
        "id": "6-PUk-jGyuCg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling NaN Values in Numerical Columns and Visualizing Numerical Data\n",
        "\n",
        "This step addresses the missing values (`NaN`) identified in the numerical columns and creates visualizations to further explore their distributions."
      ],
      "metadata": {
        "id": "q191IFqHw8QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a histogram to analyze the distribution of each numerical cols.\n",
        "plt.figure(figsize=(20, 15))\n",
        "num_rows = (len(num_cols) + 2) // 3\n",
        "no_cols = 3\n",
        "\n",
        "for index, col in enumerate(num_cols):\n",
        "    plt.subplot(num_rows, no_cols, index + 1)\n",
        "    sns.distplot(df[col])\n",
        "    plt.title(f'{col} Distribution Plot (figure {index+1})')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ox3uTfFK-CJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Based on above analysis, fill the missing values with median or mean. We used median for skewed and mean for unskewed data. As we can seee most of the cols are right skewed*"
      ],
      "metadata": {
        "id": "Yv0uDrAISDfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"normalized-losses\"].fillna(value=df[\"normalized-losses\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "56zrcRFOpz_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"wheel-base\"].fillna(value=df[\"wheel-base\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "dv2xiSc0r_y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"length\"].fillna(value=df[\"length\"].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "iaSG28cxsUzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"width\"].fillna(value=df[\"width\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "OfCeF7qJ0fUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"height\"].fillna(value=df[\"height\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "wZWrGkh_1B2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"curb-weight\"].fillna(value=df[\"curb-weight\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "xA_Jh7cg1X7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"engine-size\"].fillna(value=df[\"engine-size\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "wTbKM5KH15nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"bore\"].fillna(value=df[\"bore\"].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "y3kT_-D92Fnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"stroke\"].fillna(value=df[\"stroke\"].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "CbV5AMBA2voh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"compression-ratio\"].fillna(value=df[\"compression-ratio\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "jDHkavMZ29F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"horsepower\"].fillna(value=df[\"horsepower\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "GDYruOAb3S-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"peak-rpm\"].fillna(value=df[\"peak-rpm\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "9E7gIdZ63llQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"city-mpg\"].fillna(value=df[\"city-mpg\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "CBVYTREC4ZfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"highway-mpg\"].fillna(value=df[\"highway-mpg\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "osrCWFnP4rtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create histogram again to analyze distribution again after handle missing values.\n",
        "n_cols = len(num_cols)\n",
        "cols_per_row = 2\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))\n",
        "\n",
        "for index, col in enumerate(num_cols):\n",
        "    plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p__6diP95v4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create box plot to analyze outliers and distribution in each cols.\n",
        "n_cols = len(num_cols)\n",
        "cols_per_row = 2\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))\n",
        "\n",
        "for index, col in enumerate(num_cols):\n",
        "  plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "  sns.boxplot(df[col])\n",
        "  plt.title(f'{col} Box Plot (figure {index+1})')\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "myqT7VdX6uIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a scatter plots to identify relation between dependent and independent variables.\n",
        "n_cols = len(num_cols)\n",
        "cols_per_row = 2\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))\n",
        "\n",
        "for index, col in enumerate(num_cols):\n",
        "  plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "  sns.scatterplot(x=df[col], y=df[\"price\"], color=\"g\")\n",
        "  plt.title(f'{col} vs Price')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Price')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u2-ikgkEAQ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above charts, we can see that the cols \"wheel base\", \"width\", \"length\", \"height\", \"curve weight\", \"bore\", \"engine-size\", \"hourse power\" have a linear relation with price."
      ],
      "metadata": {
        "id": "AZNNKggHUb0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap for alanyze correlation.\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(df[num_cols].corr(numeric_only=True),annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xDvQY09MFUkC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevent numericals columns with respect to target variable\n",
        "\n",
        "less_correlation_cols = ['height', 'compression-ratio', 'stroke', 'peak-rpm', 'normalized-losses']\n",
        "df.drop(columns=less_correlation_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "_XBLWH-B3vc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch categorical columns\n",
        "\n",
        "all_cols = df.columns.tolist()\n",
        "cols_to_exclude  = num_cols\n",
        "\n",
        "cat_cols = [col for col in all_cols if col not in cols_to_exclude]"
      ],
      "metadata": {
        "id": "Qq5y7Z9FGg8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot count chart to analyze counts of each categorical cols and balancing .\n",
        "n_cols = len(cat_cols)\n",
        "cols_per_row = 2\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))\n",
        "\n",
        "for index, col in enumerate(cat_cols):\n",
        "  plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "  sns.countplot(x=df[col])\n",
        "  plt.title(col)\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oWDV_GLHLzXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pie chart to analyze categories in a cols.\n",
        "n_cols = len(cat_cols)\n",
        "cols_per_row = 1  # You can adjust this\n",
        "rows_needed = int(np.ceil(n_cols / cols_per_row))\n",
        "\n",
        "plt.figure(figsize=(15 * cols_per_row, 5 * rows_needed))  # Adjust figure size\n",
        "\n",
        "for index, col in enumerate(cat_cols):\n",
        "  plt.subplot(rows_needed, cols_per_row, index + 1)\n",
        "  value_counts = df[col].value_counts()\n",
        "  plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "  plt.title(col)\n",
        "  plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xTMoH49FPXRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring and Handling Outliers in Numerical Columns\n",
        "\n",
        "This step involves identifying outliers within the numerical features of the dataset and applying appropriate strategies to handle them. Outliers are data points that deviate significantly from other observations and can potentially skew model training and evaluation."
      ],
      "metadata": {
        "id": "M_S6vtjttAQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for outliers using the IQR\n",
        "def get_bounds(df, col):\n",
        "  Q1 = df[col].quantile(0.25)\n",
        "  Q3 = df[col].quantile(0.75)\n",
        "\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower_bound = Q1 - 1.5*IQR\n",
        "  upper_bound = Q3 + 1.5*IQR\n",
        "\n",
        "  return lower_bound, upper_bound"
      ],
      "metadata": {
        "id": "YoEXaqsGSQFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the outliers in cols\n",
        "num_cols = separate_numerial_col_by_value_count(df.select_dtypes(exclude='O').columns)\n",
        "for col in num_cols:\n",
        "  lower_bound, upper_bound = get_bounds(df, col)\n",
        "\n",
        "  outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "  print(lower_bound)\n",
        "  print(upper_bound)\n",
        "  print(f\"The {col} data has {len(outliers)} outliers\")\n",
        "  print(df[col].describe())\n",
        "  print(\"----------------------------------------\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6puxGot-VezV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_outliers(num_col):\n",
        "#   df_cleaned = df.copy()\n",
        "\n",
        "#   for col in num_col:\n",
        "#     lower_bound, upper_bound = get_bounds(df_cleaned, col)\n",
        "\n",
        "#     # Keep only the rows within bounds\n",
        "#     df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
        "\n",
        "#   return df_cleaned"
      ],
      "metadata": {
        "id": "ay9o5RzMVp7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove_outliers_cols = [\"length\", \"peak-rpm\", \"\"]\n",
        "\n",
        "# updated_df = remove_outliers(remove_outliers_cols)"
      ],
      "metadata": {
        "id": "591DtVpgYXO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appling capping on outliers\n",
        "\n",
        "def iqr_capping(updated_df, cols):\n",
        "  for col in cols:\n",
        "    lower_bound, upper_bound = get_bounds(updated_df, col)\n",
        "\n",
        "    updated_df[col] = np.where(updated_df[col] < lower_bound, lower_bound, np.where(updated_df[col] > upper_bound, upper_bound, updated_df[col]))\n",
        "  return None"
      ],
      "metadata": {
        "id": "v4EPExZG0JSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# capping_cols = [\n",
        "#     \"normalized-losses\",\n",
        "#     \"wheel-base\",\n",
        "#     \"width\",\n",
        "#     \"curb-weight\",\n",
        "#     \"engine-size\",\n",
        "#     \"bore\",\n",
        "#     \"horsepower\",\n",
        "#     \"peak-rpm\",\n",
        "#     \"city-mpg\",\n",
        "#     \"highway-mpg\",\n",
        "#     \"city-mpg\"]\n",
        "capping_cols = num_cols\n",
        "iqr_capping(df, capping_cols)"
      ],
      "metadata": {
        "id": "HfBcBExkLXwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate no outlier left\n",
        "sns.histplot(df[\"length\"], kde=True)\n",
        "plt.title(f\"Distribution of {col}\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Hx-bDoCOGPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Data into Numerical Representation\n",
        "\n",
        "This step involves converting the categorical features in our dataset into a numerical format that can be understood and processed by machine learning models. We employed either Label Encoding or One-Hot Encoding based on the nature of each categorical column.\n",
        "\n",
        "**Actions Performed:**\n",
        "\n",
        "1.  **Identify Categorical Columns for Encoding:**\n",
        "    * We selected the remaining categorical columns in the DataFrame (after potentially dropping some non-impacting ones in a previous step). These columns typically have a data type of `object`.\n",
        "\n",
        "\n",
        "2.  **Apply Encoding Techniques:**\n",
        "    * **Label Encoding:** This technique was applied to binary categorical features (those with only two unique categories). Label Encoding assigns a numerical label (e.g., 0 and 1) to each category.\n",
        "\n",
        "    * **One-Hot Encoding:** This technique was applied to multi-category nominal features (those with more than two unique categories and no inherent order). One-Hot Encoding creates new binary columns for each unique category in the original column.\n",
        "\n",
        "**Verification:**\n",
        "\n",
        "After applying the encoding techniques, the DataFrame was inspected to:\n",
        "\n",
        "* Confirm that the original categorical columns have been replaced by numerical representations (either single binary columns from Label Encoding or multiple binary columns from One-Hot Encoding).\n",
        "* Check the data types of the newly created columns (they should be numerical, typically `int64`).\n",
        "* Ensure that the number of columns has increased as expected due to One-Hot Encoding."
      ],
      "metadata": {
        "id": "kn2ZjFLqu4-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.select_dtypes(include='O').columns:\n",
        "  print(col)\n",
        "  print(df[col].unique())\n",
        "  print(\"=====================\")"
      ],
      "metadata": {
        "id": "7fCCmYIbQSPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode categorical cols, apply label when col has 2 categories else one hot.\n",
        "def encode_columns(updated_df):\n",
        "  label_encoder = LabelEncoder()\n",
        "  cols_to_one_hot = []\n",
        "  for col in updated_df.select_dtypes(include='O').columns:\n",
        "    if len(updated_df[col].unique()) == 2:\n",
        "      updated_df[col] = label_encoder.fit_transform(updated_df[col])\n",
        "      updated_df[col] = updated_df[col].astype('int16')\n",
        "\n",
        "    else:\n",
        "       cols_to_one_hot.append(col)\n",
        "  print(cols_to_one_hot)\n",
        "  updated_df = pd.get_dummies(updated_df, columns=cols_to_one_hot, dtype=int, drop_first=True)\n",
        "  return updated_df"
      ],
      "metadata": {
        "id": "A-3oTho1atTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df = encode_columns(df)"
      ],
      "metadata": {
        "id": "pFWs04CxagA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df.columns"
      ],
      "metadata": {
        "id": "95vpVdU8b8dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df.shape"
      ],
      "metadata": {
        "id": "Geey_N0BrAzd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df.head(5)"
      ],
      "metadata": {
        "id": "wj7WSmcZVs4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df.info()"
      ],
      "metadata": {
        "id": "nKs5CSUAXJ1z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Testing, and Validating the Linear Regression Model\n",
        "\n",
        "This step involves building, training, evaluating, and validating a Linear Regression model to predict car prices using the preprocessed numerical features.\n",
        "\n",
        "**Actions Performed:**\n",
        "\n",
        "1.  **Data Splitting:**\n",
        "    * The dataset was split into training and testing sets using `train_test_split` from `sklearn.model_selection`. A typical split ratio of 80% for training and 20% for testing was used. A `random_state` was set for reproducibility.\n",
        "\n",
        "2.  **Model Instantiation:**\n",
        "    * A Linear Regression model was instantiated using `LinearRegression` from `sklearn.linear_model`.\n",
        "\n",
        "\n",
        "3.  **Model Training:**\n",
        "    * The instantiated Linear Regression model was trained on the training data (`X_train`, `y_train`) using the `.fit()` method.\n",
        "\n",
        "\n",
        "4.  **Model Evaluation on Test Set:**\n",
        "    * The trained model was used to make predictions on the unseen test data (`X_test`) using the `.predict()` method.\n",
        "    * The performance of the model was evaluated using appropriate regression metrics such as:\n",
        "        * **Mean Squared Error (MSE):** Measures the average squared difference between the predicted and actual values.\n",
        "        * **R-squared (R2 Score):** Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "The evaluation metrics on the test set provide an indication of how well the Linear Regression model generalizes to unseen data. The cross-validation results offer a more stable estimate of the model's performance on the training data and can help in detecting potential overfitting or underfitting. These results serve as a baseline for comparison with other more complex models."
      ],
      "metadata": {
        "id": "Bqz2LXStwA8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "fl9uOPHovzb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have selected relevent features for linear model, they are highly correalted with price\n",
        "X = updated_df[['aspiration',\n",
        "               'engine-location',\n",
        "               'wheel-base',\n",
        "               'length',\n",
        "               'width',\n",
        "               'curb-weight',\n",
        "               'engine-size',\n",
        "               'bore',\n",
        "               'horsepower',\n",
        "               'city-mpg',\n",
        "               'highway-mpg'\n",
        "              ]]\n",
        "y = updated_df['price']"
      ],
      "metadata": {
        "id": "P5t8Kuv6W4fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)"
      ],
      "metadata": {
        "id": "ilr360wdbAEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "g3nDyzzabVFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "c_WFjnFVbbJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a constant to get an intercept\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the resgression line using 'OLS'\n",
        "lr = sm.OLS(y_train, X_train_sm).fit()"
      ],
      "metadata": {
        "id": "wXp9TqkcbvUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print params that define the increase of value and impact on price\n",
        "lr.params"
      ],
      "metadata": {
        "id": "kHlMRhr7cXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing a summary operation lists out all the different parameters of the regression line fitted\n",
        "print(lr.summary())"
      ],
      "metadata": {
        "id": "G7Asqf1QcuNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = lr.predict(X_train_sm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_train, y=y_pred_train)\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.title('Actual Price vs. Predicted Price (Training Data)')\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eLiJrwiVfeO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = lr.predict(X_train_sm)\n",
        "res = (y_train - y_train_pred)"
      ],
      "metadata": {
        "id": "0z6fgn3smI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "sns.distplot(res, bins = 15)\n",
        "fig.suptitle('Error Terms', fontsize = 15)\n",
        "plt.xlabel('y_train - y_train_pred', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdvT3IO0o1xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a constant to X_test\n",
        "X_test_sm = sm.add_constant(X_test)\n",
        "\n",
        "# Predict the y values corresponding to X_test_sm\n",
        "y_pred = lr.predict(X_test_sm)"
      ],
      "metadata": {
        "id": "EiuXNSkApYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = (y_train - y_train_pred)"
      ],
      "metadata": {
        "id": "BbxQQv3s0WRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "sns.distplot(res, bins = 15)\n",
        "fig.suptitle('Error Terms', fontsize = 15)\n",
        "plt.xlabel('y_train - y_train_pred', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ws5Y_E5u0ZKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OstfacPopmew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns the mean squared error; we'll take a square root\n",
        "np.sqrt(mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "GnBDb8NOpxre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_squared = r2_score(y_test, y_pred)\n",
        "r_squared"
      ],
      "metadata": {
        "id": "ddIXp4e8p0NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = lr.predict(X_test_sm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred_test)\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.title('Actual Price vs. Predicted Price (Training Data)')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3HDtebGzp2eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Decision Tree Regression Model\n",
        "\n",
        "This step involves building, training, and evaluating a Decision Tree Regression model for predicting car prices. Decision Trees can capture non-linear relationships in the data and might perform differently compared to Linear Regression.\n",
        "\n",
        "**Actions Performed:**\n",
        "\n",
        "1.  **Model Instantiation:**\n",
        "    * A Decision Tree Regression model was instantiated using `DecisionTreeRegressor` from `sklearn.tree`. We started with default hyperparameters, but these can be tuned later for better performance.\n",
        "\n",
        "2.  **Model Training:**\n",
        "    * The instantiated Decision Tree Regression model was trained on the training data (`X_train`, `y_train`) using the `.fit()` method.\n",
        "\n",
        "3.  **Model Evaluation on Test Set:**\n",
        "    * The trained Decision Tree model was used to make predictions on the unseen test data (`X_test`) using the `.predict()` method.\n",
        "    * The performance of the model was evaluated using the same regression metrics as Linear Regression: Mean Squared Error (MSE) and R-squared (R2 Score).\n",
        "\n",
        "4.  **Visualization of the Decision Tree:**\n",
        "    * For a better understanding of how the Decision Tree makes predictions, the trained tree can be visualized, especially for trees with a reasonable depth.\n",
        "\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "The performance metrics of the Decision Tree model on the test set were compared to those of the Linear Regression model. Decision Trees often have the potential to outperform linear models when the underlying relationships in the data are non-linear. However, they are also prone to overfitting, especially if the tree is allowed to grow very deep. The cross-validation results help in assessing the model's generalization capability. Visualizing the tree provides insights into the decision rules learned by the model. The next step would typically involve tuning the hyperparameters of the Decision Tree (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`) to optimize its performance and prevent overfitting."
      ],
      "metadata": {
        "id": "carbVTCj0qjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm, datasets"
      ],
      "metadata": {
        "id": "VBTtpBfCp5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = train_test_split(updated_df, test_size=0.20, random_state = 42)"
      ],
      "metadata": {
        "id": "vnSE8Wy8Mbkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a  regression model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model = DecisionTreeRegressor()\n",
        "#Decision Tree Regressors in sklearn require input features in a 2D array.\n",
        "x_train = data_train[['aspiration',\n",
        "               'engine-location',\n",
        "               'wheel-base',\n",
        "               'length',\n",
        "               'width',\n",
        "               'curb-weight',\n",
        "               'engine-size',\n",
        "               'bore',\n",
        "               'horsepower',\n",
        "               'city-mpg',\n",
        "               'highway-mpg'\n",
        "              ]]# if training on 2 or more columns\n",
        "y_train = np.array(data_train[\"price\"]) #1D\n",
        "model.fit(x_train,y_train) #learns from training data\n",
        "\n",
        "#test how well it works ?? test data"
      ],
      "metadata": {
        "id": "ImE28RR6OvyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_sq = model.score(x_train, y_train)\n",
        "print(f\"R2: {r_sq}\")"
      ],
      "metadata": {
        "id": "zDIyO_0UPriG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_squared_error(y_train, model.predict(x_train))"
      ],
      "metadata": {
        "id": "A5v_m1YaPvet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = data_test[['aspiration',\n",
        "               'engine-location',\n",
        "               'wheel-base',\n",
        "               'length',\n",
        "               'width',\n",
        "               'curb-weight',\n",
        "               'engine-size',\n",
        "               'bore',\n",
        "               'horsepower',\n",
        "               'city-mpg',\n",
        "               'highway-mpg'\n",
        "              ]]# if training on 2 or more columns\n",
        "y_test = np.array(data_test[\"price\"]) #1D\n",
        "\n",
        "mean_squared_error(y_test, model.predict(x_test)) #mean_squared_error(Y_true,Y_pred)"
      ],
      "metadata": {
        "id": "ABQwUBYuQM8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "a = plot_tree(model,\n",
        "              feature_names=X_train.columns.tolist(), #???\n",
        "              # class_names=y_train, #??\n",
        "              filled=True,\n",
        "              rounded=True,\n",
        "              fontsize=14)"
      ],
      "metadata": {
        "id": "AKtjdxEtQpW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# user grid search to tunr params\n",
        "\n",
        "param = {\n",
        "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'splitter': ['best', 'random']\n",
        "}\n",
        "# Performing GridSearchCV\n",
        "grid_search = GridSearchCV(model, cv=5, param_grid=param)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_estimator_)\n",
        "print(\"Best Score (R2):\", grid_search.best_score_)\n",
        "\n",
        "# Evaluating on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "test_score = best_model.score(x_test, y_test)\n",
        "print(\"Test Score (R2):\", test_score)"
      ],
      "metadata": {
        "id": "z9Vh6cWyRRNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Random Forest Regression Model\n",
        "\n",
        "This step involves building, training, and evaluating a Random Forest Regression model, which is an ensemble learning method that combines multiple decision trees to make more robust and accurate predictions. Random Forests often outperform single decision trees by reducing overfitting and improving generalization.\n",
        "\n",
        "**Actions Performed:**\n",
        "\n",
        "1.  **Model Instantiation:**\n",
        "    * A Random Forest Regression model was instantiated using `RandomForestRegressor` from `sklearn.ensemble`. We started with initial hyperparameters, but these are typically tuned using techniques like GridSearchCV or RandomizedSearchCV for optimal performance.\n",
        "\n",
        "2.  **Model Training:**\n",
        "    * The instantiated Random Forest Regression model was trained on the training data (`X_train`, `y_train`) using the `.fit()` method. Training a Random Forest involves building multiple decision trees on different subsets of the data and features.\n",
        "\n",
        "3.  **Model Evaluation on Test Set:**\n",
        "    * The trained Random Forest model was used to make predictions on the unseen test data (`X_test`) using the `.predict()` method. The predictions are typically the average of the predictions from all the individual decision trees in the forest.\n",
        "    * The performance of the model was evaluated using Mean Squared Error (MSE) and R-squared (R2 Score).\n",
        "\n",
        "4.  **Feature Importance Analysis:**\n",
        "    * Random Forests provide a measure of feature importance, indicating which features contributed most to the predictions. This can be useful for understanding the underlying relationships in the data.\n",
        "\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "The performance metrics of the Random Forest model were compared to those of Linear Regression and the single Decision Tree. Random Forests often achieve better performance due to their ability to reduce variance and handle complex relationships. The feature importance analysis provides insights into which features the model relies on most heavily for prediction. The next crucial step would be to tune the hyperparameters of the Random Forest model (e.g., `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`) using cross-validation to further optimize its performance."
      ],
      "metadata": {
        "id": "zrDpQVv34WVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "L6TVKjPWTHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = updated_df.drop('price', axis=1)\n",
        "y = updated_df['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OKkjDkLHid4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instantiate the Random Forest Regressor model\n",
        "# You can start with default hyperparameters and tune later\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# 2. Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "00NacK4Mi_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Random Forest Regressor Evaluation:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R^2): {r_squared:.2f}\")"
      ],
      "metadata": {
        "id": "sBBvvxa5jGCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the predictions vs. actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual Price vs. Predicted Price (Random Forest Regressor)\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rnkAn04njX51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importances = rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "sorted_indices = np.argsort(feature_importances)[::-1]"
      ],
      "metadata": {
        "id": "G0TzdZYujb6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Feature Importance (Random Forest Regressor)\")\n",
        "plt.bar(range(X_train.shape[1]), feature_importances[sorted_indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), feature_names[sorted_indices], rotation='vertical')\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwrANm_DjfiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=========== END =============\")"
      ],
      "metadata": {
        "id": "jpfxuSXUjhOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}